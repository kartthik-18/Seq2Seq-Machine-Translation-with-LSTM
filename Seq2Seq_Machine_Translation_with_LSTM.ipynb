{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartthik-18/Seq2Seq-Machine-Translation-with-LSTM/blob/main/Seq2Seq_Machine_Translation_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn8GBTgIdgbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ea6c4d-75ee-4b13-82eb-7dbebbbe7172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ3Mu6r_dgbo"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Input, Dense, Bidirectional, LSTM, Embedding\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LoPKZo7dgbp"
      },
      "source": [
        "### Verify access to the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fuhpMKJdgbp",
        "outputId": "1d6426b0-351c-4f56-ef75-5f3724b6d0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 12852348280800136569\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14619377664\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 12669869457634620505\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o3EHOhveitP",
        "outputId": "006e0b94-f883-4805-91cd-a64f45e6c9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'small_vocab_fr.txt', 'small_vocab_en.txt', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkrmDLz5dgbq"
      },
      "outputs": [],
      "source": [
        "with open('small_vocab_en.txt', 'r') as f:\n",
        "    eng_sentences = f.read().split('\\n')\n",
        "\n",
        "with open('small_vocab_fr.txt', 'r') as f:\n",
        "    fre_sentences = f.read().split('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr6RY5Ojdgbq",
        "outputId": "44d44957-4dd3-4002-ad62-9229169742fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Sentence 1 :  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "French Sentence 1  :  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "\n",
            "English Sentence 2 :  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "French Sentence 2  :  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "\n",
            "English Sentence 3 :  california is usually quiet during march , and it is usually hot in june .\n",
            "French Sentence 3  :  california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sample_i in range(3):\n",
        "    print('English Sentence {} :  {}'.format(sample_i+1, eng_sentences[sample_i]))\n",
        "    print('French Sentence {}  :  {}\\n'.format(sample_i+1, fre_sentences[sample_i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8jkPWdYdgbq"
      },
      "source": [
        "# 2. Pre-process text\n",
        "## 2.1. Tokenize function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCZVBFf0dgbr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "def tokenize(x, encode_start_end = False):\n",
        "\n",
        "    if encode_start_end:\n",
        "        x = [\"startofsentence \" + sentence + \" endofsentence\" for sentence in x]\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    tokenized_x = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "    return tokenized_x, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13bunExXdgbr"
      },
      "source": [
        "## 2.2. Padding  function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVds_M-rdgbr"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "\n",
        "    padded_x = pad_sequences(x, maxlen = length, padding = 'post', truncating = 'post')\n",
        "\n",
        "    return padded_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGIqstCZdgbs",
        "outputId": "6a21fa51-b4ba-44c5-f173-98ddf66b9403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size:  199\n",
            "french vocabulary size:  346\n",
            "\n",
            "Length of longest English sentence:  15\n",
            "Length of longest french sentence:  23\n",
            "\n"
          ]
        }
      ],
      "source": [
        "eng_tokenized, eng_tokenizer = tokenize(eng_sentences)\n",
        "fre_tokenized, fre_tokenizer = tokenize(fre_sentences, encode_start_end = True)\n",
        "\n",
        "eng_encoded = pad(eng_tokenized)\n",
        "fre_encoded = pad(fre_tokenized)\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index)\n",
        "fre_vocab_size = len(fre_tokenizer.word_index)\n",
        "\n",
        "print(\"English vocabulary size: \", eng_vocab_size)\n",
        "print(\"french vocabulary size: \", fre_vocab_size)\n",
        "print()\n",
        "\n",
        "eng_seq_len = len(eng_encoded[0])\n",
        "fre_seq_len = len(fre_encoded[0])\n",
        "\n",
        "print(\"Length of longest English sentence: \", eng_seq_len)\n",
        "print(\"Length of longest french sentence: \", fre_seq_len)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSkLSycgdgbs"
      },
      "source": [
        "# 3. Build Seq2Seq Model & Train\n",
        "## 3.1. Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVTKan66dgbs",
        "outputId": "9c1ce382-6cdb-475b-e8e9-e70d1e56de0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 102ms/step - accuracy: 0.2036 - loss: 2.4966 - val_accuracy: 0.1655 - val_loss: 0.6956\n",
            "Epoch 2/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 60ms/step - accuracy: 0.1644 - loss: 0.5956 - val_accuracy: 0.1647 - val_loss: 0.3790\n",
            "Epoch 3/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - accuracy: 0.1636 - loss: 0.3372 - val_accuracy: 0.1644 - val_loss: 0.1782\n",
            "Epoch 4/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.1636 - loss: 0.1439 - val_accuracy: 0.1643 - val_loss: 0.0673\n",
            "Epoch 5/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - accuracy: 0.1633 - loss: 0.0525 - val_accuracy: 0.1642 - val_loss: 0.0586\n",
            "Epoch 6/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - accuracy: 0.1634 - loss: 0.0406 - val_accuracy: 0.1642 - val_loss: 0.0354\n",
            "Epoch 7/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.1677 - loss: 0.7540 - val_accuracy: 0.1653 - val_loss: 0.4640\n",
            "Epoch 8/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.1639 - loss: 0.3545 - val_accuracy: 0.1644 - val_loss: 0.1556\n",
            "Epoch 9/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.1636 - loss: 0.1261 - val_accuracy: 0.1644 - val_loss: 0.0834\n",
            "Epoch 10/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.1634 - loss: 0.0743 - val_accuracy: 0.1643 - val_loss: 0.0597\n",
            "Epoch 11/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.1635 - loss: 0.0497 - val_accuracy: 0.1643 - val_loss: 0.0469\n",
            "Epoch 12/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.1637 - loss: 0.0406 - val_accuracy: 0.1642 - val_loss: 0.0420\n",
            "Epoch 13/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.1636 - loss: 0.0344 - val_accuracy: 0.1642 - val_loss: 0.0382\n",
            "Epoch 14/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - accuracy: 0.1638 - loss: 0.0304 - val_accuracy: 0.1643 - val_loss: 0.0355\n",
            "Epoch 15/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.1626 - loss: 0.0265 - val_accuracy: 0.1642 - val_loss: 0.0314\n",
            "Epoch 16/16\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.1638 - loss: 0.0238 - val_accuracy: 0.1642 - val_loss: 0.0299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78047088d4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "english_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "ed_french_vocab_size = len(fre_tokenizer.word_index) + 1\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "ed_preproc_english_sentences = eng_encoded\n",
        "ed_preproc_french_sentences = fre_encoded\n",
        "\n",
        "english_input = ed_preproc_english_sentences   # (137861, 15)\n",
        "\n",
        "decoder_french_input = ed_preproc_french_sentences[:, :-1]\n",
        "decoder_french_target = ed_preproc_french_sentences[:, 1:]\n",
        "\n",
        "# Reshape to (batch_size, seq_len, 1)\n",
        "decoder_french_input = np.expand_dims(decoder_french_input, -1)\n",
        "decoder_french_target = np.expand_dims(decoder_french_target, -1)\n",
        "\n",
        "# continue model definition...\n",
        "\n",
        "# Reshape to (batch_size, seq_len, 1)\n",
        "decoder_french_input = np.expand_dims(decoder_french_input, -1)\n",
        "decoder_french_target = np.expand_dims(decoder_french_target, -1)\n",
        "\n",
        "\n",
        "# 1. Define Encoder\n",
        "input_seq_encoder = Input(shape = (None, ),\n",
        "                          name = \"encoder_input\")     # (batch_size, sentence_length, 1)\n",
        "\n",
        "embed_dim = 200\n",
        "embedded_seq_encoder = Embedding(input_dim = english_vocab_size,\n",
        "                                 output_dim = embed_dim)(input_seq_encoder)\n",
        "\n",
        "encoder_lstm = LSTM(units = 256,\n",
        "                    activation = 'relu',\n",
        "                    return_sequences = False,\n",
        "                    return_state = True,\n",
        "                    name = \"encoder_LSTM\")\n",
        "\n",
        "_, last_hidden_encoder, last_cell_encoder = encoder_lstm(embedded_seq_encoder)\n",
        "\n",
        "\n",
        "# 2. Define Decoder\n",
        "input_seq_decoder = Input(shape = (None, 1),\n",
        "                          name = \"decoder_input\")     # (batch_size, sentence_length, 1)\n",
        "\n",
        "decoder_lstm = LSTM(units = 256,\n",
        "                    activation = 'relu',\n",
        "                    return_sequences = True,\n",
        "                    return_state = True,\n",
        "                    name = \"decoder_LSTM\")\n",
        "\n",
        "all_hidden_decoder, _, _ = decoder_lstm(input_seq_decoder,\n",
        "                                        initial_state = [last_hidden_encoder, last_cell_encoder])\n",
        "\n",
        "decoder_dense = Dense(ed_french_vocab_size,   # NOT TIMEDISTRIBUTED (NOT RECURSIVE)\n",
        "                      activation = 'softmax',\n",
        "                      name = \"decoder_dense\")\n",
        "logits = decoder_dense(all_hidden_decoder)\n",
        "\n",
        "\n",
        "# 3. Define Model\n",
        "final_rnn_model = Model(inputs = [input_seq_encoder, input_seq_decoder],\n",
        "                        outputs = logits)\n",
        "\n",
        "\n",
        "final_rnn_model.compile(loss = sparse_categorical_crossentropy,\n",
        "                        optimizer = Adam(learning_rate = 0.002),\n",
        "                        metrics = ['accuracy'])\n",
        "\n",
        "# 4. Fit the Model\n",
        "final_rnn_model.fit([english_input, decoder_french_input],\n",
        "                    decoder_french_target,\n",
        "                    batch_size = 1024,\n",
        "                    epochs = 16,\n",
        "                    validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "Z33HEwqQdgbs",
        "outputId": "bc5f5c4f-f2ba-4a35-d64d-ed3b463ddfb5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m) │     \u001b[38;5;34m40,000\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_LSTM (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m467,968\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_LSTM (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m264,192\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_LSTM[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_LSTM[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m347\u001b[0m) │     \u001b[38;5;34m89,179\u001b[0m │ decoder_LSTM[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">40,000</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">467,968</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">264,192</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">89,179</span> │ decoder_LSTM[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,584,019\u001b[0m (9.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,584,019</span> (9.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m861,339\u001b[0m (3.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">861,339</span> (3.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,722,680\u001b[0m (6.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,722,680</span> (6.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "final_rnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_ou2uefdgbs"
      },
      "source": [
        "## 3.2. Inference model\n",
        "### 2.1. Encoder Model for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRvsoGtydgbt"
      },
      "outputs": [],
      "source": [
        "last_states_encoder = [last_hidden_encoder, last_cell_encoder]\n",
        "inference_encoder_model = Model(inputs = input_seq_encoder,\n",
        "                                outputs = last_states_encoder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVByfvq1dgbt"
      },
      "source": [
        "### 2.2. Decoder Model for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEpvZOXGdgbt"
      },
      "outputs": [],
      "source": [
        "decoder_initial_state = [Input(shape = (256,)), Input(shape = (256,))]\n",
        "all_hidden_decoder, last_hidden_decoder, last_cell_decoder = decoder_lstm(input_seq_decoder,\n",
        "                                                                          initial_state = decoder_initial_state)\n",
        "\n",
        "logits = decoder_dense(all_hidden_decoder)\n",
        "\n",
        "inference_decoder_model = Model(inputs  = [input_seq_decoder] + decoder_initial_state,\n",
        "                                outputs = [logits, last_hidden_decoder, last_cell_decoder])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S15iFkYNdgbt"
      },
      "source": [
        "### 2.3. Decode Sequence Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u244hCFRdgbt"
      },
      "outputs": [],
      "source": [
        "target_id_to_word = {idx: word for word, idx in fre_tokenizer.word_index.items()}\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    \"\"\"\n",
        "    Gets predictions using the final model defined above\n",
        "    :param input_seq: (list) encoded english sentence (list of word ids)\n",
        "    returns : translated French sentence\n",
        "    \"\"\"\n",
        "    decoder_input = inference_encoder_model.predict(input_seq)\n",
        "\n",
        "    # Initialize decoder input as a length 1 sentence containing \"startofsentence\"\n",
        "    prev_word = np.zeros((1, 1, 1))\n",
        "    prev_word[0, 0, 0] = fre_tokenizer.word_index[\"startofsentence\"]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        # 1. predict the next word using decoder model\n",
        "        logits, last_h, last_c = inference_decoder_model.predict([prev_word] + decoder_input)\n",
        "\n",
        "        # 2. Get the predicted word\n",
        "        predicted_id = np.argmax(logits[0, 0, :])\n",
        "        predicted_word = target_id_to_word.get(predicted_id, '')\n",
        "\n",
        "        decoded_sentence.append(predicted_word)\n",
        "\n",
        "        # 3. End condition\n",
        "        if (predicted_word == 'endofsentence' or len(decoded_sentence) > decoder_french_target.shape[1]):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 4. Update decoder input and state\n",
        "        prev_word[0, 0, 0] = predicted_id\n",
        "        decoder_input = [last_h, last_c]\n",
        "\n",
        "    return \" \".join(decoded_sentence).replace('endofsentence', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB_bOWgddgbu"
      },
      "source": [
        "### 2.4. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG3L8lvNdgbu",
        "outputId": "119f7108-c5cd-489f-da3f-82be1fd2de1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "English Sentence            :  new jersey is usually quiet during fall , but it is usually warm in april .\n",
            "Predicted French Translation:  new jersey est généralement calme au cours de l' automne mais il est habituellement chaud en avril \n",
            "Correct French Translation  :  new jersey est généralement calme au cours de l' automne , mais il est généralement chaud en avril .\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "English Sentence            :  the united states is sometimes pleasant during july , but it is sometimes dry in june .\n",
            "Predicted French Translation:  les états unis est parfois agréable en juillet mais il est parfois sec en juin \n",
            "Correct French Translation  :  les états-unis est parfois agréable en juillet , mais il est parfois sec en juin .\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "English Sentence            :  she saw the new green truck .\n",
            "Predicted French Translation:  elle a vu le nouveau camion vert \n",
            "Correct French Translation  :  elle a vu le nouveau camion vert .\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "English Sentence            :  france is busy during august , and it is nice in february .\n",
            "Predicted French Translation:  france est occupé au mois d' août et il est agréable en février \n",
            "Correct French Translation  :  france est occupé au mois d' août , et il est agréable en février .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print prediction(s)\n",
        "for i in [100, 284, 380, 345]:\n",
        "    english_seq = ed_preproc_english_sentences[i].reshape(1, ed_preproc_english_sentences.shape[1])\n",
        "    french_translation = decode_sequence(english_seq)\n",
        "\n",
        "    print(\"English Sentence            : \", eng_sentences[i])\n",
        "    print(\"Predicted French Translation: \", french_translation)\n",
        "    print(\"Correct French Translation  : \", fre_sentences[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cyoCIg_u0nPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}